{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 단어의 토큰화\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "\r\n",
    "paper = ['많은 것을 바꾸고 싶다면 많은 것을 받아들여라']\r\n",
    "\r\n",
    "tknz = Tokenizer()\r\n",
    "tknz.fit_on_texts(paper)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(tknz.word_index)\r\n",
    "print(tknz.word_counts)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'많은': 1, '것을': 2, '바꾸고': 3, '싶다면': 4, '받아들여라': 5}\n",
      "OrderedDict([('많은', 2), ('것을', 2), ('바꾸고', 1), ('싶다면', 1), ('받아들여라', 1)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 원핫 인코딩\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "\r\n",
    "paper = ['많은 것을 바꾸고 싶다면 많은 것을 받아들여라']\r\n",
    "tknz = Tokenizer()\r\n",
    "tknz.fit_on_texts(paper)\r\n",
    "\r\n",
    "idx_paper = tknz.texts_to_sequences(paper)\r\n",
    "n = len(tknz.word_index)+1\r\n",
    "idx_onehot = to_categorical(idx_paper, num_classes=n)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(idx_paper)\r\n",
    "print(n)\r\n",
    "print(idx_onehot)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1, 2, 3, 4, 1, 2, 5]]\n",
      "6\n",
      "[[[0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# 단어 임베딩\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Embedding\r\n",
    "\r\n",
    "model = Sequential()\r\n",
    "model.add(Embedding(input_dim=n, output_dim=3))\r\n",
    "model.compile(optimizer='rmsprop', loss='mse')\r\n",
    "embedding = model.predict(idx_paper)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(embedding)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[ 0.01347996  0.04358232 -0.01255677]\n",
      "  [-0.00283106 -0.02775335 -0.01162326]\n",
      "  [ 0.01668438 -0.01551704  0.03676926]\n",
      "  [ 0.03386027 -0.0054024  -0.00484884]\n",
      "  [ 0.01347996  0.04358232 -0.01255677]\n",
      "  [-0.00283106 -0.02775335 -0.01162326]\n",
      "  [ 0.01882217 -0.02422711  0.01118854]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Seq2seq"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "np.random.seed(0)\r\n",
    "tf.random.set_seed(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "n_batch = 64\r\n",
    "epochs = 100\r\n",
    "latent_dim = 256\r\n",
    "n_max_sample = 10000\r\n",
    "data_path = '../rawdata/eng-fra/fra.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as f:\r\n",
    "    lines = f.read().split('\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "lines[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)',\n",
       " 'Hi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)',\n",
       " 'Hi.\\tSalut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)',\n",
       " 'Run!\\tCours\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)',\n",
       " 'Run!\\tCourez\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)',\n",
       " 'Who?\\tQui ?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux)',\n",
       " 'Wow!\\tÇa alors\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo)',\n",
       " 'Fire!\\tAu feu !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 (sacredceltic)',\n",
       " \"Help!\\tÀ l'aide\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko)\",\n",
       " 'Jump.\\tSaute.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Micsmithel)']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# 인풋, 타겟 텍스트 데이터 정리\r\n",
    "x_txts = []\r\n",
    "y_txts = []\r\n",
    "x_chars_uni = set()\r\n",
    "y_chars_uni = set()\r\n",
    "n_sample = min(n_max_sample, len(lines) - 1)    \r\n",
    "\r\n",
    "for line in lines[:n_sample]:\r\n",
    "    x_txt, y_txt, _ = line.split('\\t')\r\n",
    "    y_txt = '\\t' + y_txt + '\\n'\r\n",
    "    x_txts.append(x_txt)\r\n",
    "    y_txts.append(y_txt)\r\n",
    "    \r\n",
    "    for char in x_txt:\r\n",
    "        if char not in x_chars_uni:\r\n",
    "            x_chars_uni.add(char)\r\n",
    "    for char in y_txt:\r\n",
    "        if char not in y_chars_uni:\r\n",
    "            y_chars_uni.add(char)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "x_txts[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Go.', 'Hi.', 'Hi.', 'Run!', 'Run!']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "y_txts[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['\\tVa !\\n',\n",
       " '\\tSalut !\\n',\n",
       " '\\tSalut.\\n',\n",
       " '\\tCours\\u202f!\\n',\n",
       " '\\tCourez\\u202f!\\n']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "x_chars_uni"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{' ',\n",
       " '!',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'é'}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "y_chars_uni"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'\\t',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\xa0',\n",
       " '«',\n",
       " '»',\n",
       " 'À',\n",
       " 'Ç',\n",
       " 'É',\n",
       " 'Ê',\n",
       " 'à',\n",
       " 'â',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'ë',\n",
       " 'î',\n",
       " 'ï',\n",
       " 'ô',\n",
       " 'ù',\n",
       " 'û',\n",
       " 'œ',\n",
       " '\\u2009',\n",
       " '’',\n",
       " '\\u202f'}"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# 토큰 단위 정리\r\n",
    "x_chars_uni = sorted(list(x_chars_uni))\r\n",
    "y_chars_uni = sorted(list(y_chars_uni))\r\n",
    "n_encoder_tokens = len(x_chars_uni)\r\n",
    "n_decoder_tokens = len(y_chars_uni)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "print(\"유니크 인코더 토큰 글자 수: \", n_encoder_tokens)\r\n",
    "print(\"유니크 디코더 토큰 글자 수: \", n_decoder_tokens)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "유니크 인코더 토큰 글자 수:  71\n",
      "유니크 디코더 토큰 글자 수:  93\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "max_encoder_seq_len = 0\r\n",
    "for txt in x_txts:\r\n",
    "    txt_len = len(txt)\r\n",
    "    max_encoder_seq_len = max(txt_len, \r\n",
    "                              max_encoder_seq_len)\r\n",
    "print(\"인코더 문장내 최대 문자 수: \", max_encoder_seq_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "인코더 문장내 최대 문자 수:  15\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "max_decoder_seq_len = 0\r\n",
    "for txt in y_txts:\r\n",
    "    txt_len = len(txt)\r\n",
    "    max_decoder_seq_len = max(txt_len, \r\n",
    "                              max_decoder_seq_len)\r\n",
    "print(\"디코더 문장내 최대 문자 수: \", max_decoder_seq_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "디코더 문장내 최대 문자 수:  59\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# 토큰 별 인덱스\r\n",
    "x_token_idx = {}\r\n",
    "for idx, char in enumerate(x_chars_uni):\r\n",
    "    x_token_idx[char] = idx\r\n",
    "    \r\n",
    "y_token_idx ={}\r\n",
    "for idx, char in enumerate(y_chars_uni):\r\n",
    "    y_token_idx[char] = idx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "x_token_idx"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '$': 2,\n",
       " '%': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '0': 9,\n",
       " '1': 10,\n",
       " '2': 11,\n",
       " '3': 12,\n",
       " '5': 13,\n",
       " '6': 14,\n",
       " '7': 15,\n",
       " '8': 16,\n",
       " '9': 17,\n",
       " ':': 18,\n",
       " '?': 19,\n",
       " 'A': 20,\n",
       " 'B': 21,\n",
       " 'C': 22,\n",
       " 'D': 23,\n",
       " 'E': 24,\n",
       " 'F': 25,\n",
       " 'G': 26,\n",
       " 'H': 27,\n",
       " 'I': 28,\n",
       " 'J': 29,\n",
       " 'K': 30,\n",
       " 'L': 31,\n",
       " 'M': 32,\n",
       " 'N': 33,\n",
       " 'O': 34,\n",
       " 'P': 35,\n",
       " 'Q': 36,\n",
       " 'R': 37,\n",
       " 'S': 38,\n",
       " 'T': 39,\n",
       " 'U': 40,\n",
       " 'V': 41,\n",
       " 'W': 42,\n",
       " 'Y': 43,\n",
       " 'a': 44,\n",
       " 'b': 45,\n",
       " 'c': 46,\n",
       " 'd': 47,\n",
       " 'e': 48,\n",
       " 'f': 49,\n",
       " 'g': 50,\n",
       " 'h': 51,\n",
       " 'i': 52,\n",
       " 'j': 53,\n",
       " 'k': 54,\n",
       " 'l': 55,\n",
       " 'm': 56,\n",
       " 'n': 57,\n",
       " 'o': 58,\n",
       " 'p': 59,\n",
       " 'q': 60,\n",
       " 'r': 61,\n",
       " 's': 62,\n",
       " 't': 63,\n",
       " 'u': 64,\n",
       " 'v': 65,\n",
       " 'w': 66,\n",
       " 'x': 67,\n",
       " 'y': 68,\n",
       " 'z': 69,\n",
       " 'é': 70}"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# 영행렬 만들기\r\n",
    "encoder_x_data = np.zeros(\r\n",
    "                            (len(x_txts), \r\n",
    "                             max_encoder_seq_len, \r\n",
    "                             n_encoder_tokens),\r\n",
    "                        dtype='float32')\r\n",
    "decoder_x_data = np.zeros(\r\n",
    "                            (len(x_txts), \r\n",
    "                             max_decoder_seq_len, \r\n",
    "                             n_decoder_tokens),\r\n",
    "                        dtype='float32')\r\n",
    "decoder_y_data = np.zeros(\r\n",
    "                            (len(x_txts), \r\n",
    "                             max_decoder_seq_len, \r\n",
    "                             n_decoder_tokens),\r\n",
    "                        dtype='float32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# 인풋 데이터 행렬\r\n",
    "for i, x_txt in enumerate(x_txts):\r\n",
    "    \r\n",
    "    for t, char in enumerate(x_txt):\r\n",
    "        encoder_x_data[i, t, x_token_idx[char]] = 1.\r\n",
    "    encoder_x_data[i, t + 1:, x_token_idx[' ']] = 1."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# 타깃 데이터 행렬\r\n",
    "for i, y_txt in enumerate(y_txts):\r\n",
    "       \r\n",
    "    for t, char in enumerate(y_txt):\r\n",
    "        decoder_x_data[i, t, y_token_idx[char]] = 1.\r\n",
    "        if t > 0:\r\n",
    "            decoder_y_data[i, t - 1, y_token_idx[char]] = 1.\r\n",
    "            \r\n",
    "    decoder_x_data[i, t + 1:, y_token_idx[' ']] = 1.\r\n",
    "    decoder_y_data[i, t:, y_token_idx[' ']] = 1."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# 인코더\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras.layers import Embedding, Input, LSTM, Dense, TimeDistributed\r\n",
    "\r\n",
    "encoder_inputs = Input(shape=(None, n_encoder_tokens))\r\n",
    "encoder = LSTM(latent_dim, return_state=True)\r\n",
    "encoder_outs, state_h, state_c = encoder(encoder_inputs)\r\n",
    "encoder_states = [state_h, state_c]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "print(encoder_inputs.shape)\r\n",
    "print(encoder_outs.shape)\r\n",
    "print(state_h.shape)\r\n",
    "print(state_c.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(None, None, 71)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 256)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# 디코더\r\n",
    "decoder_inputs = Input(shape=(None, n_decoder_tokens))\r\n",
    "decoder = LSTM(latent_dim, \r\n",
    "                return_sequences=True, \r\n",
    "                return_state=True)\r\n",
    "decoder_outs, _, _ = decoder(decoder_inputs,\r\n",
    "                             initial_state=encoder_states)\r\n",
    "decoder_dense = TimeDistributed(Dense(n_decoder_tokens, \r\n",
    "                                      activation='softmax'))\r\n",
    "decoder_outputs = decoder_dense(decoder_outs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "print(decoder_inputs.shape)\r\n",
    "print(decoder_outputs.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(None, None, 93)\n",
      "(None, None, 93)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# 인코더 - 디코더\r\n",
    "model = Model([encoder_inputs, decoder_inputs], \r\n",
    "              decoder_outputs)\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 71)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 93)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 335872      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  358400      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 93)     23901       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 718,173\n",
      "Trainable params: 718,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# 모형 컴파일\r\n",
    "model.compile(optimizer='rmsprop', \r\n",
    "              loss='categorical_crossentropy',\r\n",
    "              metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "model.fit([encoder_x_data, decoder_x_data], decoder_y_data,\r\n",
    "          batch_size=n_batch,\r\n",
    "          epochs=epochs,\r\n",
    "          validation_split=0.2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 47s 6ms/sample - loss: 1.1878 - accuracy: 0.7248 - val_loss: 1.0634 - val_accuracy: 0.7013\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 45s 6ms/sample - loss: 0.8538 - accuracy: 0.7689 - val_loss: 0.8344 - val_accuracy: 0.7683\n",
      "Epoch 3/100\n",
      "7936/8000 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.8082"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 추론 모형 생성\r\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\r\n",
    "\r\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\r\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\r\n",
    "decoder_states_inputs = [decoder_state_input_h, \r\n",
    "                         decoder_state_input_c]\r\n",
    "decoder_outputs, state_h, state_c = decoder(\r\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\r\n",
    "decoder_states = [state_h, state_c]\r\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\r\n",
    "decoder_model = Model(\r\n",
    "    [decoder_inputs] + decoder_states_inputs,\r\n",
    "    [decoder_outputs] + decoder_states)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 리버스 인덱스\r\n",
    "reverse_x_char_idx = {}\r\n",
    "for char, idx in x_token_idx.items():\r\n",
    "    reverse_x_char_idx[idx] = char\r\n",
    "    \r\n",
    "reverse_y_char_idx ={}\r\n",
    "for char, idx in y_token_idx.items():\r\n",
    "    reverse_y_char_idx[idx] = char"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reverse_x_char_idx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 결과값 디코딩\r\n",
    "def decode_sequence(input_seq):\r\n",
    "    states_value = encoder_model.predict(input_seq)\r\n",
    "    y_seq = np.zeros((1, 1, n_decoder_tokens))\r\n",
    "    y_seq[0, 0, y_token_idx['\\t']] = 1.\r\n",
    "\r\n",
    "    stop_condition = False\r\n",
    "    decoded_sentence = ''\r\n",
    "    while not stop_condition:\r\n",
    "        output_tokens, h, c = decoder_model.predict(\r\n",
    "            [y_seq] + states_value)\r\n",
    "\r\n",
    "        # Sample a token\r\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\r\n",
    "        sampled_char = reverse_y_char_idx[sampled_token_index]\r\n",
    "        decoded_sentence += sampled_char\r\n",
    "\r\n",
    "        if (sampled_char == '\\n' or\r\n",
    "           len(decoded_sentence) > max_decoder_seq_len):\r\n",
    "            stop_condition = True\r\n",
    "\r\n",
    "        y_seq = np.zeros((1, 1, n_decoder_tokens))\r\n",
    "        y_seq[0, 0, sampled_token_index] = 1.\r\n",
    "\r\n",
    "        states_value = [h, c]\r\n",
    "\r\n",
    "    return decoded_sentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 결과확인\r\n",
    "for seq_idx in range(100):\r\n",
    "    x_seq = encoder_x_data[seq_idx: seq_idx + 1]\r\n",
    "    decoded_sentence = decode_sequence(x_seq)\r\n",
    "    print('-')\r\n",
    "    print('Input sentence:', x_txts[seq_idx])\r\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('py3_7_6': conda)"
  },
  "interpreter": {
   "hash": "0c27090090f508e4883836458afaa8253b2ac5af7b12b60af14ac7c2616a5b0c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
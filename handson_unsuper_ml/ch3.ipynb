{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "'''메인 라이브러리'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, pickle, gzip\n",
    "\n",
    "'''시각화 관련 라이브러리'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "\n",
    "'''데이터 준비 관련 라이브러리'''\n",
    "from sklearn import preprocessing as pp \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 로드\n",
    "current_path = os.getcwd()\n",
    "file = os.path.sep.join(['', 'datasets', 'mnist_data', 'mnist.pkl.gz'])\n",
    "\n",
    "f = gzip.open(current_path+file, 'rb')\n",
    "train_set, validation_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_validation, y_validation = validation_set[0], validation_set[1]\n",
    "X_test, y_test = test_set[0], test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 구조 확인\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of X_validation: \", X_validation.shape)\n",
    "print(\"Shape of y_validation: \", y_validation.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋으로부터 판다스 데이터 프레임 만들기\n",
    "train_index = range(0,len(X_train))\n",
    "validation_index = range(len(X_train), \\\n",
    "                         len(X_train)+len(X_validation))\n",
    "test_index = range(len(X_train)+len(X_validation), \\\n",
    "                   len(X_train)+len(X_validation)+len(X_test))\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
    "y_train = pd.Series(data=y_train,index=train_index)\n",
    "\n",
    "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
    "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
    "\n",
    "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
    "y_test = pd.Series(data=y_test,index=test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터의 요약 결과 생성\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 데이터 보기\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_digit(example):\n",
    "    label = y_train.loc[example]\n",
    "    image = X_train.loc[example, :].values.reshape([28, 28])\n",
    "    plt.title('Example : %d Label : %d' % (example, label))\n",
    "    plt.imshow(image, cmap = plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 이미지 살펴보기\n",
    "view_digit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(series):\n",
    "    label_binarizer = pp.LabelBinarizer()\n",
    "    label_binarizer.fit(range(max(series)+1))\n",
    "    return label_binarizer.transform(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot(originalSeries, newSeries):\n",
    "    label_binarizer = pp.LabelBinarizer()\n",
    "    label_binarizer.fit(range(max(originalSeries)+1))\n",
    "    return label_binarizer.inverse_transform(newSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블에 대한 원-핫 벡터 생성\n",
    "y_train_oneHot = one_hot(y_train)\n",
    "y_validation_oneHot = one_hot(y_validation)\n",
    "y_test_oneHot = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 예제 데이터 숫자 5에 대한 원-핫 벡터 보기\n",
    "y_train_oneHot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주성분 분석\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 784\n",
    "whiten = False\n",
    "random_state = 2018\n",
    "\n",
    "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
    "          random_state=random_state)\n",
    "\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 784개의 주성분으로 추출한 원본 데이터의 분산 비율\n",
    "\n",
    "print(\"Variance Explained by all 784 principal components: \", \\\n",
    "      sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X개의 주성분으로 추출한 원본 데이터의 분산 비율\n",
    "\n",
    "importanceOfPrincipalComponents = \\\n",
    "    pd.DataFrame(data=pca.explained_variance_ratio_)\n",
    "importanceOfPrincipalComponents = importanceOfPrincipalComponents.T\n",
    "\n",
    "print('Variance Captured by First 10 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:9].sum(axis=1).values)\n",
    "print('Variance Captured by First 20 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:19].sum(axis=1).values)\n",
    "print('Variance Captured by First 50 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:49].sum(axis=1).values)\n",
    "print('Variance Captured by First 100 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:99].sum(axis=1).values)\n",
    "print('Variance Captured by First 200 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:199].sum(axis=1).values)\n",
    "print('Variance Captured by First 300 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:299].sum(axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "sns.barplot(data=importanceOfPrincipalComponents.loc[:,0:9],color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterPlot(xDF, yDF, algoName):\n",
    "    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n",
    "    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n",
    "    tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
    "    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n",
    "               data=tempDF, fit_reg=False)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Separation of Observations using \"+algoName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterPlot(X_train_PCA, y_train, \"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scatter = pd.DataFrame(data=X_train.loc[:,[350,406]], index=X_train.index)\n",
    "X_train_scatter = pd.concat((X_train_scatter,y_train), axis=1, join=\"inner\")\n",
    "X_train_scatter.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
    "sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", data=X_train_scatter, fit_reg=False)\n",
    "ax = plt.gca()\n",
    "ax.set_title(\"Separation of Observations Using Original Feature Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 점진적 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_components = 784\n",
    "batch_size = None\n",
    "\n",
    "incrementalPCA = IncrementalPCA(n_components=n_components, \\\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "X_train_incrementalPCA = incrementalPCA.fit_transform(X_train)\n",
    "X_train_incrementalPCA = \\\n",
    "    pd.DataFrame(data=X_train_incrementalPCA, index=train_index)\n",
    "\n",
    "X_validation_incrementalPCA = incrementalPCA.transform(X_validation)\n",
    "X_validation_incrementalPCA = \\\n",
    "    pd.DataFrame(data=X_validation_incrementalPCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_incrementalPCA, y_train, \"Incremental PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 희소 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "n_components = 100\n",
    "alpha = 0.0001\n",
    "random_state = 2018\n",
    "n_jobs = -1\n",
    "\n",
    "# 수정 사항 : normalize_components='deprecated' 옵션을 설정하면 경고메세지가 삭제됨 components 정규화는 무조건 실행 되기 때문에 의미 없는 옵션임\n",
    "sparsePCA = SparsePCA(n_components=n_components, \\\n",
    "                alpha=alpha, random_state=random_state, n_jobs=n_jobs,normalize_components='deprecated')\n",
    "\n",
    "sparsePCA.fit(X_train.loc[:10000,:])\n",
    "X_train_sparsePCA = sparsePCA.transform(X_train)\n",
    "X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=train_index)\n",
    "\n",
    "X_validation_sparsePCA = sparsePCA.transform(X_validation)\n",
    "X_validation_sparsePCA = \\\n",
    "    pd.DataFrame(data=X_validation_sparsePCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_sparsePCA, y_train, \"Sparse PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 커널 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "n_components = 100\n",
    "kernel = 'rbf'\n",
    "gamma = None\n",
    "random_state = 2018\n",
    "n_jobs = 1\n",
    "\n",
    "kernelPCA = KernelPCA(n_components=n_components, kernel=kernel, \\\n",
    "                      gamma=gamma, n_jobs=n_jobs, random_state=random_state)\n",
    "\n",
    "kernelPCA.fit(X_train.loc[:10000,:])\n",
    "X_train_kernelPCA = kernelPCA.transform(X_train)\n",
    "X_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA,index=train_index)\n",
    "\n",
    "X_validation_kernelPCA = kernelPCA.transform(X_validation)\n",
    "X_validation_kernelPCA = \\\n",
    "    pd.DataFrame(data=X_validation_kernelPCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_kernelPCA, y_train, \"Kernel PCA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특잇값 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_components = 200\n",
    "algorithm = 'randomized'\n",
    "n_iter = 5\n",
    "random_state = 2018\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components, algorithm=algorithm, \\\n",
    "                   n_iter=n_iter, random_state=random_state)\n",
    "\n",
    "X_train_svd = svd.fit_transform(X_train)\n",
    "X_train_svd = pd.DataFrame(data=X_train_svd, index=train_index)\n",
    "\n",
    "X_validation_svd = svd.transform(X_validation)\n",
    "X_validation_svd = pd.DataFrame(data=X_validation_svd, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_svd, y_train, \"Singular Value Decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤투영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRP - eps는 JL-Lemma에 따라 임베딩 품질 제어하며 이 값이 작을 수록 차원의 수가 높아짐\n",
    "\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "n_components = 'auto'\n",
    "eps = 0.5\n",
    "random_state = 2018\n",
    "\n",
    "GRP = GaussianRandomProjection(n_components=n_components, eps=eps, \\\n",
    "                               random_state=random_state)\n",
    "\n",
    "X_train_GRP = GRP.fit_transform(X_train)\n",
    "X_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)\n",
    "\n",
    "X_validation_GRP = GRP.transform(X_validation)\n",
    "X_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_GRP, y_train, \"Gaussian Random Projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRP \n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "n_components = 'auto'\n",
    "density = 'auto'\n",
    "eps = 0.5\n",
    "dense_output = False\n",
    "random_state = 2018\n",
    "\n",
    "SRP = SparseRandomProjection(n_components=n_components, \\\n",
    "        density=density, eps=eps, dense_output=dense_output, \\\n",
    "        random_state=random_state)\n",
    "\n",
    "X_train_SRP = SRP.fit_transform(X_train)\n",
    "X_train_SRP = pd.DataFrame(data=X_train_SRP, index=train_index)\n",
    "\n",
    "X_validation_SRP = SRP.transform(X_validation)\n",
    "X_validation_SRP = pd.DataFrame(data=X_validation_SRP, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_SRP, y_train, \"Sparse Random Projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IsoMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "n_neighbors = 5\n",
    "n_components = 10\n",
    "n_jobs = 4\n",
    "\n",
    "isomap = Isomap(n_neighbors=n_neighbors, \\\n",
    "                n_components=n_components, n_jobs=n_jobs)\n",
    "\n",
    "isomap.fit(X_train.loc[0:5000,:])\n",
    "X_train_isomap = isomap.transform(X_train)\n",
    "X_train_isomap = pd.DataFrame(data=X_train_isomap, index=train_index)\n",
    "\n",
    "X_validation_isomap = isomap.transform(X_validation)\n",
    "X_validation_isomap = pd.DataFrame(data=X_validation_isomap, \\\n",
    "                                   index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_isomap, y_train, \"Isomap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "n_components = 2\n",
    "n_init = 12\n",
    "max_iter = 1200\n",
    "metric = True\n",
    "n_jobs = 4\n",
    "random_state = 2018\n",
    "\n",
    "mds = MDS(n_components=n_components, n_init=n_init, max_iter=max_iter, \\\n",
    "          metric=metric, n_jobs=n_jobs, random_state=random_state)\n",
    "\n",
    "X_train_mds = mds.fit_transform(X_train.loc[0:1000,:])\n",
    "X_train_mds = pd.DataFrame(data=X_train_mds, index=train_index[0:1001])\n",
    "\n",
    "scatterPlot(X_train_mds, y_train, \"Multidimensional Scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE(Locally Linear Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "n_neighbors = 10\n",
    "n_components = 2\n",
    "method = 'modified'\n",
    "n_jobs = 4\n",
    "random_state = 2018\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, \\\n",
    "        n_components=n_components, method=method, \\\n",
    "        random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "lle.fit(X_train.loc[0:5000,:])\n",
    "X_train_lle = lle.transform(X_train)\n",
    "X_train_lle = pd.DataFrame(data=X_train_lle, index=train_index)\n",
    "\n",
    "X_validation_lle = lle.transform(X_validation)\n",
    "X_validation_lle = pd.DataFrame(data=X_validation_lle, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_lle, y_train, \"Locally Linear Embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_un)",
   "language": "python",
   "name": "conda_un"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
